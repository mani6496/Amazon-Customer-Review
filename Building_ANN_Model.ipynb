{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwDuNT6pH2BvA/vpiXfXXR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mani6496/Amazon-Customer-Review/blob/main/Building_ANN_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "B9y4AfqBkoxI"
      },
      "outputs": [],
      "source": [
        "#Import @equire Library\n",
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##Load Mnist Dataset\n",
        "(x_train,y_train),(x_test,y_test)=mnist.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAinKTV_lze5",
        "outputId": "c45fe3e4-7e81-41d8-b006-27152b497519"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoTOlsRHmhP1",
        "outputId": "52142170-6e2a-4e3b-aeb1-5e7daf274912"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "60000"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepocess the data(Flatten the images & normalize the pixel values)\n",
        "x_train=x_train.reshape(-1, 784).astype('float32')/255.0\n",
        "x_test=x_test.reshape(-1, 784).astype('float32')/255.0\n"
      ],
      "metadata": {
        "id": "TxcdrK6imx76"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QWN40ajlpII4",
        "outputId": "e397bd30-bb6a-463a-b1a1-a70a00e1f73e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([5, 0, 4, ..., 5, 6, 8], dtype=uint8)"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Convert the labels to one-hot encoded vectors\n",
        "y_train=tf.one_hot(y_train, depth=10).numpy()\n",
        "y_test=tf.one_hot(y_test, depth=10).numpy()"
      ],
      "metadata": {
        "id": "_HSAXGRZomY1"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XJfSKJX9pufc",
        "outputId": "88a0cde2-6335-4a62-877b-38625643dba8"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [1., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 1., 0.]], dtype=float32)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Number of input,Hidden, Output\n",
        "n_input = 784\n",
        "n_hidden = 250\n",
        "n_output = 10"
      ],
      "metadata": {
        "id": "2ixFcxTcp56X"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the weights & bias hidden units\n",
        "w1=tf.Variable(tf.random.normal([n_input,n_hidden]))\n",
        "b1=tf.Variable(tf.zeros([n_hidden]))"
      ],
      "metadata": {
        "id": "JRjXHOshq5zX"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the weights & bias for the output layer\n",
        "w2=tf.Variable(tf.random.normal([n_hidden,n_output]))\n",
        "b2=tf.Variable(tf.zeros([n_output]))"
      ],
      "metadata": {
        "id": "qh3MCn74sdCh"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the Forward Pass through the network\n",
        "def neural_net(x):\n",
        "  # Pass the input through the first layer\n",
        "    z1=tf.matmul(x,w1)+b1\n",
        "    a1=tf.nn.relu(z1)\n",
        "    # Pass from hidden to output layer\n",
        "    z2=tf.matmul(a1,w2)+b2\n",
        "    y=tf.nn.softmax(z2)\n",
        "    return y\n"
      ],
      "metadata": {
        "id": "zPRk1bFQtbbM"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the loss function (correct-predicted=loss)(categorical cross entropy)\n",
        "def categorical_cross_entropy(y_pred,y_true):\n",
        "   epsilon = 1e-7\n",
        "   y_pred=tf.clip_by_value(y_pred,epsilon,1-epsilon)\n",
        "   cce=-tf.reduce_sum(y_true*tf.math.log(y_pred),axis=1)\n",
        "   return tf.reduce_mean(cce)\n"
      ],
      "metadata": {
        "id": "-7J0ZB_qBv14"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the optimization alogo(Adam)\n",
        "optimizer=tf.optimizers.Adam(learning_rate=0.001)"
      ],
      "metadata": {
        "id": "wqAY7MPPDiWh"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Define the model for a fixed number of epochs\n",
        "batch_size=128\n",
        "n_epochs=20\n",
        "n_batches=x_train.shape[0] //batch_size\n",
        "\n",
        "for i in range(n_epochs):\n",
        "    for j in range(n_batches):\n",
        "        #Get the current batch of training data\n",
        "        start_idx = j * batch_size\n",
        "        end_idx = start_idx + batch_size # Define end_idx here\n",
        "        x_batch=x_train[start_idx:end_idx]\n",
        "        y_batch=y_train[start_idx:end_idx]\n",
        "\n",
        "        with tf.GradientTape() as tape:\n",
        "             # Forward pass through the network\n",
        "             y_pred = neural_net(x_batch)\n",
        "\n",
        "             #compute the loss\n",
        "             loss = categorical_cross_entropy(y_pred,y_batch)\n",
        "        #Compute the gradients of the loss with respect to the model parameters\n",
        "        grads=tape.gradient(loss, [w1,b1,w2,b2])\n",
        "\n",
        "        # Update the model parameters using the gradients\n",
        "        optimizer.apply_gradients(zip(grads, [w1,b1,w2,b2]))\n",
        "    #Evaluate the model on the test data\n",
        "    y_pred = neural_net(x_test)\n",
        "    accuracy=tf.reduce_mean(tf.cast(tf.equal(tf.argmax(y_pred,axis=-1), tf.argmax(y_test, axis=-1)), tf.float32))\n",
        "    print(f\"Epoch {i+1}: Test accuracy={accuracy.numpy():.3f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68zU5xeiD9oH",
        "outputId": "8817cd0f-3370-4728-c059-b380dcb0a569"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1: Test accuracy=0.604\n",
            "Epoch 2: Test accuracy=0.784\n",
            "Epoch 3: Test accuracy=0.807\n",
            "Epoch 4: Test accuracy=0.815\n",
            "Epoch 5: Test accuracy=0.825\n",
            "Epoch 6: Test accuracy=0.833\n",
            "Epoch 7: Test accuracy=0.835\n",
            "Epoch 8: Test accuracy=0.920\n",
            "Epoch 9: Test accuracy=0.927\n",
            "Epoch 10: Test accuracy=0.931\n",
            "Epoch 11: Test accuracy=0.932\n",
            "Epoch 12: Test accuracy=0.936\n",
            "Epoch 13: Test accuracy=0.938\n",
            "Epoch 14: Test accuracy=0.938\n",
            "Epoch 15: Test accuracy=0.941\n",
            "Epoch 16: Test accuracy=0.941\n",
            "Epoch 17: Test accuracy=0.944\n",
            "Epoch 18: Test accuracy=0.944\n",
            "Epoch 19: Test accuracy=0.946\n",
            "Epoch 20: Test accuracy=0.947\n"
          ]
        }
      ]
    }
  ]
}